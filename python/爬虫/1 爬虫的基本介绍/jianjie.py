1.什么是爬虫?
'''
n名词  网络蜘蛛  网络机器人  web spider
v动词 通过模拟真实用户 向服务器(发送请求) >>获取响应(爬取服务器的数据)>>数据清洗(提取数据)的 程序

爬虫本质  可见皆可爬

重点  在于能否获取到真实的数据  >>>  服务器会识别你是否是一个真实的用户    不是/反扒   >>>不返回数据/返回假数据
爬虫是程序
涉及概念 反反爬
默认是以 requests库的身份发送网络请求的     服务器会检测基本字段 进行反扒
useragent  用户身份   (在程序里面数属于字段)
手动在请求的位置 加上useragent (目的在于模拟用户身份发送请求)

反扒2
cookie反扒   cookie确认用户身份的字段     (在程序里面数属于字段)
手动在请求的位置 加上cookie (目的在于模拟真实用户身份发送请求)

反扒3
referer反扒   referer防盗链 代表本次请求是从referer位置跳转过来的   (在程序里面数属于字段)
手动在请求的位置 加上referer(目的在于模拟真实用户身份发送请求)

反扒4
ip反扒
请求的次数 频率很高   会被反扒
获取 代理ip(帮助我们发送请求)
'''
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
2.为什么要学习 爬虫？
'''
外包 专门到公司 做爬虫业务的数据
帮助用户

和数据分析师  大数据  （推送） 分析数据（情感分析：对关键字进行分析看 关键字是好还是坏的）
哪个区 哪家店的好评高   给你推送   带动地区GDP增长

数据从哪里来？
爬虫工程师进行采集    某团  某zhong点评

'''
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
3.爬虫的分类
"""
1.通用爬虫
    什么是通用爬虫  什么都能进行采集的爬虫
    百度就是一个通用爬虫(搜索引擎) 采集关键字相关的数据  所有网页都可以采集

2.聚焦爬虫
    是我们要神父学习的 知识点
    针对性进行数据采集的爬虫    针对单张图片 单个视频  音频
    2.1 累积式爬虫   从开始到结束一直爬取  中间做到去重的操作 （一次性的 内存释放以后(程序结束以后再次运行会重复爬取的爬虫)）
    2.2 增量式爬虫   从开始到结束一直爬取  中间做到去重的操作  (非一次性的  会读取本地的文件 看这个文件是否存在 在/不采集,不在/采集)

    2.3深层网络意义爬虫   重点  难点
        username : 132132，
        password : 585858




"""
# 正常情况下 传递给服务器确认身份使用的 参数
data={
    "username":"16416554160",
    "password":"4564646"
}
# 被加密以后   一种反扒手段
data2={
    "username":"hkfahifhakifhkjahfkajhlkfjhksafhpwoihfgpiqhfquip",
    "password":"hgoiqwhoghbqighbqibvjdngjipterjhiuwhtiqbiugbwqeughbwqeouhgwehg"
}

# 反反爬手段   需要讲 加密账户密码的 加密函数找到 通过代码实现加密密文的方法  在通过调用生成密文的函数(接口) 生成密文
# 构建到data字典里面发送请求 获取响应
# 这就是js逆向

# 运行内存运行
def crawl_data():
    # 存储url的列表
    lista=[]
    print("采集网易云")
    print("通过集合 去重音乐链接")
    print("保存音乐")

crawl_data()



""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
4.爬虫的学习大
'''
1.http协议   爬虫协议  清楚如何构建协议   (可以被量化成代码的)就是键值对  字典

2.requests库/模块  发送http协议的库
    两种方法
    2.1 requests.get方法  专门采集网页上的静态资源的方法   图片，音乐，文字，视频
    2.2 requests.post方法  专门采集确认用户身份的数据/或者叫必须要提交表单的数据

3.获取响应以后  对数据进行处理
    3.1图片音视频 /直接保存
    3.2文本类型数据
        非结构化数据 html格式              lxml xpath    或者 暴力提取数据的方法  re 正则 re.findall()/.*?/()
        结构化数据  json格式(字典)         jsonpath

4.初级反扒 (反扒，反反爬处理措施)  加上对应的请求字段
    4.1 useragent 用户代理反扒
    4.2 cookie  确认用户身份字段反扒
    4.3 referer  监测请求来源反扒

5.js逆向 难点 重点 高级反反扒 /  js加密是反扒
    爬虫需要找到 表单加密的位置   加密函数并且把js代码复制到 本地 进行执行 生成对应的密文    再次进行请求
    md5 base64 rsa ob混淆

6.视频反扒
    某站视频采集
    反扒  音视频分离    要么只能获取 音视 或者视频
    反反爬措施    分别获取音视频  通过ffmpeg软件进行音视频 合成

7.selenium 自动化测试工具
    做什么   可以完成用户在浏览器 进行操作的操作   获取网页源代码(音视频图片的链接，文本数据)

8.fiddler fiddler是一个抓包软件   可以抓取移动端的数据包   使用模拟器(运行在电脑上的手机)进行抓包

9.字体反扒
     字体反反爬措施    通过密文  到字体文件的映射表映射出明文

10.scrapy框架  高并发多线程进行大批量数据采集的框架
'''
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
5.爬虫的流程
"""
1.确认url (涉及网页数据包获取) 抓包
2.发送请求  获取响应
3.清洗数据      >>>>>  提取url  >>>再次发送请求
4.保存数据

简单的采集网页上的 图片
"""
# 发送请求的库
import requests

# 1.确认url (涉及网页数据包获取) 抓包
url="https://img1.baidu.com/it/u=444306396,3319778759&fm=253&app=138&size=w931&n=0&f=JPEG&fmt=auto?sec=1664038800&t=56824a4cbe52851a4883ff0716d17454"
# 2.发送请求  获取响应
response=requests.get(url)
# 图片音频视频 不需要进行清洗 直接进行保存  因为他们是二进制流的数据 bytes 类型数据
print(response.content)
# wb以字节类型写入
with open("小姐姐.png","wb")as file1:
    #已写入字节数据类型写入 小姐姐.png文件里面
    file1.write(response.content)
# tips    pycharm 只支持图片文件预览  不支持 MP3 mp4 文件类型的预览
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
6.robots协议
"""
机器人协议 robots协议
约定了 在这个网站下  哪些爬虫可以来采集数据 哪些爬虫不能来采集数据    严格意义上来讲仅仅是一种单方面的协议

查看网站下的robots协议
通过在导航栏输入
域名/robots.txt
https://www.csdn.net/robots.txt

User-agent: *
代表 所有的爬虫都不能来采集 我下面的数据
Disallow: /scripts
代表不能采集 网站下面的  xx数据

"""












































